import os
import re
import streamlit as st
from datetime import date, datetime
import yt_dlp
import pandas as pd
import random
import json
from googleapiclient.discovery import build
from openai import OpenAI

# --- –ö–ª—é—á—ñ API (—Ç–µ–ø–µ—Ä –∑—ñ Streamlit Secrets) ---
OPENAI_API_KEY = st.secrets.get("OPENAI_API_KEY")
YT_API_KEY = st.secrets.get("YOUTUBE_API_KEY")

# –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –∫–ª—ñ—î–Ω—Ç–∞ OpenAI
if not OPENAI_API_KEY:
    st.error("–ö–ª—é—á OpenAI API (OPENAI_API_KEY) –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ —É Streamlit Secrets. –ë—É–¥—å –ª–∞—Å–∫–∞, –¥–æ–¥–∞–π—Ç–µ –π–æ–≥–æ.")
    client = None
else:
    client = OpenAI(api_key=OPENAI_API_KEY)

# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∫–ª—é—á–∞ YouTube Data API
if not YT_API_KEY:
    st.warning(
        "–ö–ª—é—á YouTube Data API (YOUTUBE_API_KEY) –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ —É Streamlit Secrets. –§—É–Ω–∫—Ü—ñ—ó, —â–æ –π–æ–≥–æ –ø–æ—Ç—Ä–µ–±—É—é—Ç—å, –º–æ–∂—É—Ç—å –Ω–µ –ø—Ä–∞—Ü—é–≤–∞—Ç–∏.")

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è —Å—Ç–æ—Ä—ñ–Ω–∫–∏ (–º–∞—î –±—É—Ç–∏ –ø–µ—Ä—à–æ—é –∫–æ–º–∞–Ω–¥–æ—é Streamlit)
st.set_page_config(page_title="YouTube Analytics Agent", layout="wide")

st.title("YouTube Analytics Agent")
st.markdown(
    """
    –Ü–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∏–π –∞–≥–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É YouTube-–∫–∞–Ω–∞–ª—ñ–≤ —Ç–∞ –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤.
    """
)

# --- –§—É–Ω–∫—Ü—ñ—ó ---

def extract_channel_id(url_input):
    """–í–∏—Ç—è–≥—É—î ID –∫–∞–Ω–∞–ª—É –∞–±–æ —ñ–º'—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ –∑ URL."""
    match_user = re.search(r"(?:https?://)?(?:www\.)?youtube\.com/@([^/?]+)", url_input)
    if match_user:
        return match_user.group(1)
    match_channel = re.search(r"(?:https?://)?(?:www\.)?youtube\.com/channel/([^/?]+)", url_input)
    if match_channel:
        return match_channel.group(1)
    return None


def fetch_video_metadata(channel_id_or_user, start_date, end_date, limit=10, show_all=False):
    """
    –ó–±–∏—Ä–∞—î –º–µ—Ç–∞–¥–∞–Ω—ñ –≤—ñ–¥–µ–æ –∑ –∫–∞–Ω–∞–ª—É –∞–±–æ –≤—ñ–¥ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é yt_dlp.
    –§—ñ–ª—å—Ç—Ä—É—î –∑–∞ –¥–∞—Ç–æ—é.
    –ü—Ä–∏–º—ñ—Ç–∫–∞: —Ü—è —Ñ—É–Ω–∫—Ü—ñ—è –Ω–∞—Ä–∞–∑—ñ –Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –≤ –æ—Å–Ω–æ–≤–Ω–æ–º—É –ø–æ—Ç–æ—Ü—ñ UI.
    """
    if not channel_id_or_user:
        st.error("–ù–µ –Ω–∞–¥–∞–Ω–æ ID –∫–∞–Ω–∞–ª—É –∞–±–æ —ñ–º'—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞.")
        return []

    if channel_id_or_user.startswith("UC"):
        url = f"https://www.youtube.com/channel/{channel_id_or_user}/videos"
        st.info(f"–°–ø—Ä–æ–±–∞ –∑–±–æ—Ä—É –≤—ñ–¥–µ–æ –¥–ª—è ID –∫–∞–Ω–∞–ª—É: {channel_id_or_user}.")
    else:
        url = f"https://www.youtube.com/@{channel_id_or_user}/videos"

    opts_flat = {
        'ignoreerrors': True, 'skip_download': True, 'extract_flat': 'discard_in_playlist',
        'dump_single_json': True, 'playlistend': limit if not show_all else None,
        'quiet': True, 'no_warnings': True,
    }
    video_ids = []
    try:
        with yt_dlp.YoutubeDL(opts_flat) as ydl:
            info = ydl.extract_info(url, download=False)
        if info and 'entries' in info and info['entries']:
            video_ids = [e['id'] for e in info['entries'] if e and e.get('id')]
        elif info and info.get('id') and not info.get('entries'):
             st.warning(f"URL {url} —Å—Ö–æ–∂–∏–π –Ω–∞ URL –æ–¥–Ω–æ–≥–æ –≤—ñ–¥–µ–æ. –¶—è —Ñ—É–Ω–∫—Ü—ñ—è –æ—á—ñ–∫—É—î URL –∫–∞–Ω–∞–ª—É.")
             return []
    except Exception as e:
        st.error(f"–ü–æ–º–∏–ª–∫–∞ yt_dlp (flat-–ø–∞—Ä—Å–∏–Ω–≥) –¥–ª—è {url}: {e}")
        return []
    if not video_ids:
        st.warning(f"–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ ID –≤—ñ–¥–µ–æ –¥–ª—è {url} –Ω–∞ –ø–µ—Ä—à–æ–º—É –µ—Ç–∞–ø—ñ.")
        return []

    opts_det = {
        'ignoreerrors': True, 'skip_download': True, 'extract_flat': False,
        'quiet': True, 'no_warnings': True,
    }
    videos = []
    video_ids_to_process = video_ids
    if not show_all and len(video_ids) > limit * 1.5:
        video_ids_to_process = video_ids[:int(limit * 1.5)]

    for vid_id in video_ids_to_process:
        if not show_all and len(videos) >= limit: break
        vurl = f"https://www.youtube.com/watch?v={vid_id}"
        try:
            with yt_dlp.YoutubeDL(opts_det) as ydl:
                vinfo = ydl.extract_info(vurl, download=False)
            if not vinfo: continue
            upload_date_str = vinfo.get('upload_date')
            publish_date = datetime.strptime(upload_date_str, '%Y%m%d').date() if upload_date_str else None
            if show_all or (publish_date and start_date <= publish_date <= end_date):
                videos.append({
                    'title': vinfo.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∏'), 'views': vinfo.get('view_count', 0),
                    'likes': vinfo.get('like_count'), 'comments_count': vinfo.get('comment_count'),
                    'duration': vinfo.get('duration', 0), 'publish_date': publish_date,
                    'thumbnail_url': vinfo.get('thumbnail'), 'url': vinfo.get('webpage_url', vurl)
                })
        except Exception as e:
            st.warning(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –æ–±—Ä–æ–±–∏—Ç–∏ –≤—ñ–¥–µ–æ {vurl}: {e}")
    return videos


def fetch_comments(video_url, pct_str="100%"):
    if not YT_API_KEY:
        st.error("–ù–µ –∑–∞–¥–∞–Ω–∏–π –∫–ª—é—á API YouTube (YOUTUBE_API_KEY) –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤.")
        return []
    video_id_match = re.search(r"(?:v=|youtu\.be/|shorts/|embed/|watch\?v=|\&v=)([\w-]{11})", video_url)
    if not video_id_match:
        video_id = video_url if re.match(r"^[\w-]{11}$", video_url) else None
        if not video_id:
            st.error(f"–ù–µ–≤—ñ—Ä–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç URL –∞–±–æ ID –≤—ñ–¥–µ–æ –¥–ª—è –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤: {video_url}")
            return []
    else:
        video_id = video_id_match.group(1)

    try:
        youtube_service = build("youtube", "v3", developerKey=YT_API_KEY)
        comments_data = []
        next_page_token = None
        max_comments_limit = 1000
        while True:
            response = youtube_service.commentThreads().list(
                part="snippet,replies", videoId=video_id, maxResults=100,
                pageToken=next_page_token, textFormat="plainText"
            ).execute()
            for item in response.get("items", []):
                top_level_comment_snippet = item.get("snippet", {}).get("topLevelComment", {}).get("snippet", {})
                replies_data = item.get("replies", {}).get("comments", [])
                comments_data.append({
                    "text": top_level_comment_snippet.get("textDisplay", ""),
                    "likes": top_level_comment_snippet.get("likeCount", 0),
                    "replies": replies_data
                })
                if len(comments_data) >= max_comments_limit: break
            if len(comments_data) >= max_comments_limit or not response.get("nextPageToken"):
                break
            next_page_token = response.get("nextPageToken")
        
        if pct_str != "100%":
            try:
                percentage_to_fetch = float(pct_str.strip('%')) / 100
                num_to_return = int(len(comments_data) * percentage_to_fetch)
                random.shuffle(comments_data) 
                return comments_data[:num_to_return]
            except ValueError:
                st.warning(f"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç –≤—ñ–¥—Å–æ—Ç–∫–∞: {pct_str}. –ü–æ–≤–µ—Ä—Ç–∞—é –≤—Å—ñ –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ.")
        return comments_data
    except Exception as e:
        st.error(f"–ü–æ–º–∏–ª–∫–∞ YouTube API –ø—Ä–∏ –æ—Ç—Ä–∏–º–∞–Ω–Ω—ñ –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤: {e}")
        return []

def gpt_sentiment_analysis(comments_texts_list, model="gpt-3.5-turbo"):
    if not client: return {"positive": 0, "neutral": 0, "negative": 0, "error": "OpenAI client not initialized."}
    if not comments_texts_list: return {"positive": 0, "neutral": 0, "negative": 0, "error": "–ù–µ–º–∞—î —Ç–µ–∫—Å—Ç—ñ–≤ –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤."}
    clean_comments_list = [c for c in comments_texts_list if isinstance(c, str) and c.strip()]
    if not clean_comments_list: return {"positive": 0, "neutral": 0, "negative": 0, "error": "–ù–µ–º–∞—î –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤ –ø—ñ—Å–ª—è –æ—á–∏—Å—Ç–∫–∏."}
    sample_comments = clean_comments_list[:100]
    prompt_text = f"""–ü—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–π –Ω–∞—Å—Ç—É–ø–Ω—ñ —é—Ç—É–±-–∫–æ–º–µ–Ω—Ç–∞—Ä—ñ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é. –ü–æ—Ä–∞—Ö—É–π —Ç–∞ –ø–æ–≤–µ—Ä–Ω–∏ –ø—Ä–∏–±–ª–∏–∑–Ω—É –∫—ñ–ª—å–∫—ñ—Å—Ç—å:
- –ø–æ–∑–∏—Ç–∏–≤–Ω–∏—Ö (positive)
- –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–∏—Ö (neutral)
- –Ω–µ–≥–∞—Ç–∏–≤–Ω–∏—Ö (negative)
–ü–æ–≤–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —É —Ñ–æ—Ä–º–∞—Ç—ñ JSON –∑ –∫–ª—é—á–∞–º–∏ "positive", "neutral", "negative". –ù–∞–ø—Ä–∏–∫–ª–∞–¥: {{"positive": X, "neutral": Y, "negative": Z}}
–ö–æ–º–µ–Ω—Ç–∞—Ä—ñ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É:\n{chr(10).join(sample_comments)}""".strip()
    try:
        response = client.chat.completions.create(
            model=model, messages=[{"role": "user", "content": prompt_text}],
            temperature=0.2, max_tokens=150)
        api_response_text = response.choices[0].message.content.strip()
        try:
            if api_response_text.startswith("```json"): api_response_text = api_response_text[7:]
            if api_response_text.endswith("```"): api_response_text = api_response_text[:-3]
            sentiment_results = json.loads(api_response_text)
            if not all(k in sentiment_results and isinstance(sentiment_results[k], int) for k in ["positive", "neutral", "negative"]):
                return {"positive": 0, "neutral": 0, "negative": 0, "error": f"GPT JSON —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –Ω–µ–≤—ñ—Ä–Ω–∞: {api_response_text}"}
            return sentiment_results
        except json.JSONDecodeError: # –†–µ–∑–µ—Ä–≤–Ω–∏–π –ø–∞—Ä—Å–∏–Ω–≥
            st.warning(f"JSON –ø–∞—Ä—Å–∏–Ω–≥ GPT –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –Ω–µ –≤–¥–∞–≤—Å—è, —Å–ø—Ä–æ–±–∞ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥—É: {api_response_text}")
            results_fallback = {"positive": 0, "neutral": 0, "negative": 0}
            for line in api_response_text.lower().splitlines():
                num = re.findall(r"\d+", line)
                if not num: continue
                if "positive" in line: results_fallback["positive"] = int(num[0])
                elif "neutral" in line: results_fallback["neutral"] = int(num[0])
                elif "negative" in line: results_fallback["negative"] = int(num[0])
            if sum(results_fallback.values()) > 0: return results_fallback
            return {"positive": 0, "neutral": 0, "negative": 0, "error": f"GPT –≤—ñ–¥–ø–æ–≤—ñ–¥—å –Ω–µ —Ä–æ–∑–ø—ñ–∑–Ω–∞–Ω–∞: {api_response_text}"}
    except Exception as e:
        return {"positive": 0, "neutral": 0, "negative": 0, "error": str(e)}

def gpt_topic_analysis_with_sentiment(comments_texts_list, model="gpt-3.5-turbo"):
    """–ê–Ω–∞–ª—ñ–∑—É—î –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ, –≤–∏–¥—ñ–ª—è—î 5 —Ç–µ–º, —ó—Ö –ø—ñ–¥—Å—É–º–æ–∫ —Ç–∞ —Ç–æ–Ω–∞–ª—å–Ω—ñ—Å—Ç—å."""
    if not client: return [{"topic": "–ü–æ–º–∏–ª–∫–∞", "summary": "–ö–ª—ñ—î–Ω—Ç OpenAI –Ω–µ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–π.", "sentiment": "negative"}]
    if not comments_texts_list: return []
    
    clean_comments_list = [c for c in comments_texts_list if isinstance(c, str) and c.strip()]
    if not clean_comments_list: return []

    sample_size = min(200, len(clean_comments_list)) # –ë—ñ–ª—å—à–∞ –≤–∏–±—ñ—Ä–∫–∞ –¥–ª—è —Ç–µ–º
    sample_for_topics = random.sample(clean_comments_list, sample_size)

    prompt_text = f"""
–ü—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–π –Ω–∞–¥–∞–Ω—É –≤–∏–±—ñ—Ä–∫—É –∑ {sample_size} –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤ –¥–æ YouTube-–≤—ñ–¥–µ–æ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é.
–¢–≤–æ—î –∑–∞–≤–¥–∞–Ω–Ω—è - –≤–∏–∑–Ω–∞—á–∏—Ç–∏ 5 –æ—Å–Ω–æ–≤–Ω–∏—Ö —Ç–µ–º –∞–±–æ –≥—Ä—É–ø –¥—É–º–æ–∫, —â–æ –æ–±–≥–æ–≤–æ—Ä—é—é—Ç—å—Å—è –≤ —Ü–∏—Ö –∫–æ–º–µ–Ω—Ç–∞—Ä—è—Ö.
–î–ª—è –∫–æ–∂–Ω–æ—ó –∑ 5 —Ç–µ–º:
1.  –î–∞–π –∫–æ—Ä–æ—Ç–∫—É, –≤–ª—É—á–Ω—É –Ω–∞–∑–≤—É —Ç–µ–º—ñ (2-4 —Å–ª–æ–≤–∞).
2.  –ù–∞–ø–∏—à–∏ –∫–æ—Ä–æ—Ç–∫–∏–π –ø—ñ–¥—Å—É–º–æ–∫ —Ü—ñ—î—ó —Ç–µ–º–∏, —â–æ –≤—ñ–¥–æ–±—Ä–∞–∂–∞—î —Å—É—Ç—å –æ–±–≥–æ–≤–æ—Ä–µ–Ω—å (1-2 —Ä–µ—á–µ–Ω–Ω—è).
3.  –í–∏–∑–Ω–∞—á –∑–∞–≥–∞–ª—å–Ω—É —Ç–æ–Ω–∞–ª—å–Ω—ñ—Å—Ç—å —Ü—ñ—î—ó —Ç–µ–º–∏: "positive", "neutral" –∞–±–æ "negative".

–ü–æ–≤–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —É —Ñ–æ—Ä–º–∞—Ç—ñ JSON-—Å–ø–∏—Å–∫—É –æ–±'—î–∫—Ç—ñ–≤. –ö–æ–∂–µ–Ω –æ–±'—î–∫—Ç –º–∞—î –º—ñ—Å—Ç–∏—Ç–∏ –∫–ª—é—á—ñ "topic" (–Ω–∞–∑–≤–∞ —Ç–µ–º–∏), "summary" (–ø—ñ–¥—Å—É–º–æ–∫) —Ç–∞ "sentiment" (—Ç–æ–Ω–∞–ª—å–Ω—ñ—Å—Ç—å).
–ü—Ä–∏–∫–ª–∞–¥ –æ–¥–Ω–æ–≥–æ –æ–±'—î–∫—Ç—É –≤ —Å–ø–∏—Å–∫—É:
  {{
    "topic": "–Ø–∫—ñ—Å—Ç—å –∑–≤—É–∫—É",
    "summary": "–ë–∞–≥–∞—Ç–æ –≥–ª—è–¥–∞—á—ñ–≤ –≤—ñ–¥–∑–Ω–∞—á–∏–ª–∏ –≤–∏—Å–æ–∫—É —è–∫—ñ—Å—Ç—å –∑–≤—É–∫—É —É –≤—ñ–¥–µ–æ, —â–æ –ø–æ–∫—Ä–∞—â–∏–ª–æ –∑–∞–≥–∞–ª—å–Ω–µ –≤—Ä–∞–∂–µ–Ω–Ω—è.",
    "sentiment": "positive"
  }}

–ö–æ–º–µ–Ω—Ç–∞—Ä—ñ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É:
{chr(10).join(sample_for_topics)}
""".strip()
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt_text}],
            temperature=0.5,
            max_tokens=1500 # –ë—ñ–ª—å—à–µ —Ç–æ–∫–µ–Ω—ñ–≤ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É —Ç–µ–º
        )
        api_response_text = response.choices[0].message.content.strip()
        try:
            if api_response_text.startswith("```json"): api_response_text = api_response_text[7:]
            if api_response_text.endswith("```"): api_response_text = api_response_text[:-3]
            topics_data = json.loads(api_response_text)
            if not isinstance(topics_data, list) or not all(
                isinstance(t, dict) and all(k in t for k in ["topic", "summary", "sentiment"]) for t in topics_data
            ):
                st.warning(f"GPT –ø–æ–≤–µ—Ä–Ω—É–≤ JSON –¥–ª—è —Ç–µ–º, –∞–ª–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –Ω–µ–≤—ñ—Ä–Ω–∞: {api_response_text}")
                return [{"topic": "–ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É", "summary": "–ù–µ –≤–¥–∞–ª–æ—Å—è —Ä–æ–∑—ñ–±—Ä–∞—Ç–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –≤—ñ–¥ GPT.", "sentiment": "negative"}]
            return topics_data[:5] # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ –º–∞–∫—Å–∏–º—É–º 5 —Ç–µ–º
        except json.JSONDecodeError:
            st.error(f"–ù–µ –≤–¥–∞–ª–æ—Å—è —Ä–æ–∑–ø–∞—Ä—Å–∏—Ç–∏ JSON –≤—ñ–¥–ø–æ–≤—ñ–¥—å –≤—ñ–¥ GPT –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É —Ç–µ–º: {api_response_text}")
            return [{"topic": "–ü–æ–º–∏–ª–∫–∞ JSON", "summary": "GPT –ø–æ–≤–µ—Ä–Ω—É–≤ –Ω–µ–≤–∞–ª—ñ–¥–Ω–∏–π JSON.", "sentiment": "negative"}]
    except Exception as e:
        st.error(f"–ü–æ–º–∏–ª–∫–∞ GPT –ø—Ä–∏ –∞–Ω–∞–ª—ñ–∑—ñ —Ç–µ–º: {e}")
        return [{"topic": "–ü–æ–º–∏–ª–∫–∞ GPT", "summary": str(e), "sentiment": "negative"}]

def gpt_analyze_comment_popularity(comment_text, replies_texts_list, model="gpt-3.5-turbo"):
    """–ê–Ω–∞–ª—ñ–∑—É—î –æ–∫—Ä–µ–º–∏–π –∫–æ–º–µ–Ω—Ç–∞—Ä —Ç–∞ –π–æ–≥–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ, —â–æ–± –ø—Ä–∏–ø—É—Å—Ç–∏—Ç–∏ –ø—Ä–∏—á–∏–Ω—É –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—ñ."""
    if not client: return "–ö–ª—ñ—î–Ω—Ç OpenAI –Ω–µ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–π."
    if not comment_text: return "–¢–µ–∫—Å—Ç –∫–æ–º–µ–Ω—Ç–∞—Ä—è –ø–æ—Ä–æ–∂–Ω—ñ–π."

    replies_str = "\n".join([f"- {r}" for r in replies_texts_list[:5]]) # –ë–µ—Ä–µ–º–æ –¥–æ 5 –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π
    
    prompt_text = f"""
–ü—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–π –Ω–∞—Å—Ç—É–ø–Ω–∏–π –∫–æ–º–µ–Ω—Ç–∞—Ä –∑ YouTube —Ç–∞ –π–æ–≥–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ (—è–∫—â–æ —î). 
–ö–æ–º–µ–Ω—Ç–∞—Ä: "{comment_text}"

–í—ñ–¥–ø–æ–≤—ñ–¥—ñ –ø—ñ–¥ –Ω–∏–º (–¥–æ 5):
{replies_str if replies_texts_list else "–í—ñ–¥–ø–æ–≤—ñ–¥–µ–π –Ω–µ–º–∞—î."}

–ù–∞ –æ—Å–Ω–æ–≤—ñ —Ç–µ–∫—Å—Ç—É –∫–æ–º–µ–Ω—Ç–∞—Ä—è —Ç–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π, –∫–æ—Ä–æ—Ç–∫–æ (1-3 —Ä–µ—á–µ–Ω–Ω—è) –≤–∏—Å–ª–æ–≤–∏ –≥—ñ–ø–æ—Ç–µ–∑—É, —á–æ–º—É —Ü–µ–π –∫–æ–º–µ–Ω—Ç–∞—Ä –º—ñ–≥ —Å—Ç–∞—Ç–∏ –ø–æ–ø—É–ª—è—Ä–Ω–∏–º (–Ω–∞–±—Ä–∞–≤ –±–∞–≥–∞—Ç–æ –ª–∞–π–∫—ñ–≤). 
–ó–≤–µ—Ä–Ω–∏ —É–≤–∞–≥—É –Ω–∞ –º–æ–∂–ª–∏–≤—ñ –ø—Ä–∏—á–∏–Ω–∏: –≥—É–º–æ—Ä, –≤–ª—É—á–Ω—ñ—Å—Ç—å, –∑–≥–æ–¥–∞ –∑ –±—ñ–ª—å—à—ñ—Å—Ç—é, –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—ñ—Å—Ç—å –¥—É–º–∫–∏, –∞–∫—Ç—É–∞–ª—å–Ω—ñ—Å—Ç—å, –ø—Ä–æ–≤–æ–∫–∞—Ç–∏–≤–Ω—ñ—Å—Ç—å, –∑–∞–ø–∏—Ç–∞–Ω–Ω—è, —â–æ –≤–∏–∫–ª–∏–∫–∞–ª–æ –¥–∏—Å–∫—É—Å—ñ—é, —Ç–æ—â–æ.
–í—ñ–¥–ø–æ–≤—ñ–¥—å –¥–∞–π —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é.
""".strip()
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt_text}],
            temperature=0.6,
            max_tokens=200 
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        st.error(f"–ü–æ–º–∏–ª–∫–∞ GPT –ø—Ä–∏ –∞–Ω–∞–ª—ñ–∑—ñ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—ñ –∫–æ–º–µ–Ω—Ç–∞—Ä—è: {e}")
        return f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ GPT: {e}"


def gpt_comment_summary(comments_texts_list, model="gpt-3.5-turbo"):
    if not client: return "–ö–ª—ñ—î–Ω—Ç OpenAI –Ω–µ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–π."
    if not comments_texts_list: return "–ù–µ–º–∞—î –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤ –¥–ª—è –ø—ñ–¥—Å—É–º–∫—É."
    clean_comments_list = [c for c in comments_texts_list if isinstance(c, str) and c.strip()]
    if not clean_comments_list: return "–ù–µ–º–∞—î –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤ –ø—ñ—Å–ª—è –æ—á–∏—Å—Ç–∫–∏ –¥–ª—è –ø—ñ–¥—Å—É–º–∫—É."
    sample_size = min(100, len(clean_comments_list))
    sample_for_summary = random.sample(clean_comments_list, sample_size)
    prompt_text = f"""–¢–æ–±—ñ –Ω–∞–¥–∞–Ω–æ –≤–∏–±—ñ—Ä–∫—É –∑ {sample_size} –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤ –ø—ñ–¥ –≤—ñ–¥–µ–æ –∑ YouTube —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é.
1. –ù–∞–ø–∏—à–∏ –∫–æ—Ä–æ—Ç–∫–∏–π –∞–Ω–∞–ª—ñ–∑ –Ω–∞–π–ø–æ–ø—É–ª—è—Ä–Ω—ñ—à–∏—Ö —Ç–µ–º –∞–±–æ –Ω–∞—Å—Ç—Ä–æ—ó–≤ —É —Ü–∏—Ö –∫–æ–º–µ–Ω—Ç–∞—Ä—è—Ö (2-3 —Ä–µ—á–µ–Ω–Ω—è).
2. –£–∑–∞–≥–∞–ª—å–Ω–∏ (–ø–æ 1-2 —Ä–µ—á–µ–Ω–Ω—è –Ω–∞ –∫–æ–∂–µ–Ω –ø—É–Ω–∫—Ç):
   - –©–æ –Ω–∞–π–±—ñ–ª—å—à–µ —Å–ø–æ–¥–æ–±–∞–ª–æ—Å—å –≥–ª—è–¥–∞—á–∞–º (—è–∫—â–æ —Ü–µ –≤–∏–¥–Ω–æ –∑ –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤)?
   - –©–æ –∑–∞–ª–∏—à–∏–ª–æ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–µ –≤—Ä–∞–∂–µ–Ω–Ω—è –∞–±–æ –±—É–ª–æ –º–µ–Ω—à –æ–±–≥–æ–≤–æ—Ä—é–≤–∞–Ω–∏–º?
   - –©–æ –≤–∏–∫–ª–∏–∫–∞–ª–æ –Ω–µ–≥–∞—Ç–∏–≤ —á–∏ –∫—Ä–∏—Ç–∏–∫—É (—è–∫—â–æ —î)?
3. –ó—Ä–æ–±–∏ –∫–æ—Ä–æ—Ç–∫–∏–π –≤–∏—Å–Ω–æ–≤–æ–∫ (1-2 —Ä–µ—á–µ–Ω–Ω—è) –ø—Ä–æ –∑–∞–≥–∞–ª—å–Ω–µ –≤—Ä–∞–∂–µ–Ω–Ω—è –∞—É–¥–∏—Ç–æ—Ä—ñ—ó.
–ö–æ–º–µ–Ω—Ç–∞—Ä—ñ:\n{chr(10).join(sample_for_summary)}""".strip()
    try:
        response = client.chat.completions.create(
            model=model, messages=[{"role": "user", "content": prompt_text}],
            temperature=0.7, max_tokens=800)
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ GPT (–ø—ñ–¥—Å—É–º–æ–∫): {e}"

def format_duration(seconds_total):
    if not isinstance(seconds_total, (int, float)) or seconds_total < 0: return "00:00:00"
    h, rem = divmod(int(seconds_total), 3600)
    m, s = divmod(rem, 60)
    return f"{h:02}:{m:02}:{s:02}"

# --- –û—Å–Ω–æ–≤–Ω–∞ –ª–æ–≥—ñ–∫–∞ –ø—Ä–æ–≥—Ä–∞–º–∏ ---
video_url_input = st.text_input("URL –≤—ñ–¥–µ–æ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É:", placeholder="–í—Å—Ç–∞–≤—Ç–µ URL YouTube –≤—ñ–¥–µ–æ...", key="main_video_url_input")
comments_percentage_options = {"–í—Å—ñ": "100%", "50%": "50%", "25%": "25%", "10%": "10%"}
selected_display_percentage = st.selectbox(
    "–Ø–∫—É —á–∞—Å—Ç–∫—É –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤ –∞–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏ (–≤–∏–ø–∞–¥–∫–æ–≤–∞ –≤–∏–±—ñ—Ä–∫–∞):",
    list(comments_percentage_options.keys()), index=0, key="comments_percentage_selector")
percentage_to_fetch_str = comments_percentage_options[selected_display_percentage]

if st.button("üöÄ –ü—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –≤—ñ–¥–µ–æ", key="analyze_this_video_button"):
    if not video_url_input:
        st.error("–ë—É–¥—å –ª–∞—Å–∫–∞, –≤–≤–µ–¥—ñ—Ç—å URL –≤—ñ–¥–µ–æ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É.")
    else:
        with st.spinner("–ó–±—ñ—Ä –¥–∞–Ω–∏—Ö –ø—Ä–æ –≤—ñ–¥–µ–æ..."):
            video_details = None
            try:
                ydl_opts_video = {'quiet': True, 'no_warnings': True, 'skip_download': True, 'ignoreerrors': True, 'extract_flat': False}
                with yt_dlp.YoutubeDL(ydl_opts_video) as ydl:
                    video_details = ydl.extract_info(video_url_input, download=False)
            except Exception as e:
                st.error(f"–ü–æ–º–∏–ª–∫–∞ yt_dlp (–¥–∞–Ω—ñ –≤—ñ–¥–µ–æ): {e}")
        if not video_details:
            st.error(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –¥–ª—è –≤—ñ–¥–µ–æ: {video_url_input}")
        else:
            st.subheader(f"–ê–Ω–∞–ª—ñ–∑ –≤—ñ–¥–µ–æ: {video_details.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∏')}")
            col_thumb, col_info = st.columns([1, 3])
            with col_thumb:
                st.image(video_details.get('thumbnail', 'https://placehold.co/240x180/CCCCCC/FFFFFF?text=No+Image'), width=240, caption="–û–±–∫–ª–∞–¥–∏–Ω–∫–∞ –≤—ñ–¥–µ–æ")
            with col_info:
                st.markdown(f"**–ù–∞–∑–≤–∞:** {video_details.get('title', 'N/A')}")
                st.markdown(f"**–¢—Ä–∏–≤–∞–ª—ñ—Å—Ç—å:** {format_duration(video_details.get('duration', 0))}")
                st.markdown(f"**–ü–µ—Ä–µ–≥–ª—è–¥–∏:** {video_details.get('view_count', 0):,}")
                st.markdown(f"**–õ–∞–π–∫–∏:** {video_details.get('like_count', 0):,}")
                st.markdown(f"**–ö–æ–º–µ–Ω—Ç–∞—Ä—ñ (yt-dlp):** {video_details.get('comment_count', 'N/A'):,}")
                upload_date_str = video_details.get('upload_date')
                if upload_date_str:
                    try: st.markdown(f"**–î–∞—Ç–∞ –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó:** {datetime.strptime(upload_date_str, '%Y%m%d').strftime('%d.%m.%Y')}")
                    except ValueError: st.markdown(f"**–î–∞—Ç–∞ –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó:** {upload_date_str} (–Ω–µ —Ä–æ–∑–ø–∞—Ä—Å–µ–Ω–æ)")
                else: st.markdown("**–î–∞—Ç–∞ –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó:** N/A")
            st.markdown("---")
            st.subheader("üìà –ê–Ω–∞–ª—ñ–∑ –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤ –¥–æ –≤—ñ–¥–µ–æ")
            with st.spinner(f"–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–∞ –∞–Ω–∞–ª—ñ–∑ ~{selected_display_percentage} –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤..."):
                fetched_comments_data = fetch_comments(video_url_input, pct_str=percentage_to_fetch_str)

            if not fetched_comments_data:
                st.info("–ö–æ–º–µ–Ω—Ç–∞—Ä—ñ –¥–æ —Ü—å–æ–≥–æ –≤—ñ–¥–µ–æ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ, –Ω–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏, –∞–±–æ –æ–±—Ä–∞–Ω–æ 0%.")
            else:
                st.info(f"–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ —Ç–∞ –±—É–¥–µ –ø—Ä–æ–∞–Ω–∞–ª—ñ–∑–æ–≤–∞–Ω–æ –ø—Ä–∏–±–ª–∏–∑–Ω–æ {len(fetched_comments_data)} –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤.")
                comment_texts_for_gpt = [c.get("text", "") for c in fetched_comments_data if isinstance(c, dict) and isinstance(c.get("text"), str) and c.get("text").strip()]

                if not comment_texts_for_gpt:
                    st.warning("–¢–µ–∫—Å—Ç–æ–≤–∏–π –≤–º—ñ—Å—Ç –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É –≤—ñ–¥—Å—É—Ç–Ω—ñ–π –∞–±–æ –ø–æ—Ä–æ–∂–Ω—ñ–π.")
                else:
                    # 1. –ê–Ω–∞–ª—ñ–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—ñ –∑–∞–≥–∞–ª–æ–º
                    st.markdown("##### –¢–æ–Ω–∞–ª—å–Ω—ñ—Å—Ç—å –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤ (–∑–∞–≥–∞–ª—å–Ω–∞ –æ—Ü—ñ–Ω–∫–∞ GPT):")
                    with st.spinner("GPT –∞–Ω–∞–ª—ñ–∑—É—î –∑–∞–≥–∞–ª—å–Ω—É —Ç–æ–Ω–∞–ª—å–Ω—ñ—Å—Ç—å..."):
                         sentiment_gpt_result = gpt_sentiment_analysis(comment_texts_for_gpt)
                    
                    total_sentiments = sum(v for k,v in sentiment_gpt_result.items() if k != "error" and isinstance(v,int))
                    if total_sentiments > 0:
                        pos_pct = (sentiment_gpt_result.get('positive', 0) / total_sentiments) * 100
                        neu_pct = (sentiment_gpt_result.get('neutral', 0) / total_sentiments) * 100
                        neg_pct = (sentiment_gpt_result.get('negative', 0) / total_sentiments) * 100
                        bar_len = 20
                        pos_bar = "üü©" * int(bar_len * pos_pct / 100)
                        neu_bar = "üü®" * int(bar_len * neu_pct / 100)
                        neg_bar = "üü•" * max(0, bar_len - len(pos_bar) - len(neu_bar))
                        st.markdown(f"{pos_bar}{neu_bar}{neg_bar} –ü–æ–∑: {pos_pct:.0f}% | –ù–µ–π—Ç: {neu_pct:.0f}% | –ù–µ–≥: {neg_pct:.0f}%")
                    if sentiment_gpt_result.get("error"):
                        st.caption(f"–ü—Ä–∏–º—ñ—Ç–∫–∞ (—Ç–æ–Ω–∞–ª—å–Ω—ñ—Å—Ç—å): {sentiment_gpt_result['error']}")
                    elif total_sentiments == 0 and "error" not in sentiment_gpt_result:
                        st.info("GPT –Ω–µ –∑–º—ñ–≥ –≤–∏–∑–Ω–∞—á–∏—Ç–∏ —á—ñ—Ç–∫—É —Ç–æ–Ω–∞–ª—å–Ω—ñ—Å—Ç—å –¥–ª—è –Ω–∞–¥–∞–Ω–æ—ó –≤–∏–±—ñ—Ä–∫–∏ –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤.")

                    # 2. –ê–Ω–∞–ª—ñ–∑ 5 –æ—Å–Ω–æ–≤–Ω–∏—Ö —Ç–µ–º
                    st.markdown("##### üí¨ –û—Å–Ω–æ–≤–Ω—ñ —Ç–µ–º–∏ –≤ –∫–æ–º–µ–Ω—Ç–∞—Ä—è—Ö (–∞–Ω–∞–ª—ñ–∑ GPT):")
                    with st.spinner("GPT –≤–∏–¥—ñ–ª—è—î –æ—Å–Ω–æ–≤–Ω—ñ —Ç–µ–º–∏ –≤ –∫–æ–º–µ–Ω—Ç–∞—Ä—è—Ö..."):
                        topic_analysis_results = gpt_topic_analysis_with_sentiment(comment_texts_for_gpt)
                    
                    if topic_analysis_results:
                        for topic_item in topic_analysis_results:
                            sentiment_color = {"positive": "green", "neutral": "orange", "negative": "red"}.get(topic_item.get("sentiment", "neutral"), "grey")
                            st.markdown(f"""
                            <div style="border-left: 5px solid {sentiment_color}; padding-left: 10px; margin-bottom: 10px;">
                                <strong>–¢–µ–º–∞: {topic_item.get('topic', '–ù–µ –≤–∏–∑–Ω–∞—á–µ–Ω–æ')}</strong> (<span style="color:{sentiment_color};">{topic_item.get('sentiment', 'N/A')}</span>)<br>
                                <em>{topic_item.get('summary', '–û–ø–∏—Å –≤—ñ–¥—Å—É—Ç–Ω—ñ–π.')}</em>
                            </div>
                            """, unsafe_allow_html=True)
                    else:
                        st.info("–ù–µ –≤–¥–∞–ª–æ—Å—è –≤–∏–¥—ñ–ª–∏—Ç–∏ –æ—Å–Ω–æ–≤–Ω—ñ —Ç–µ–º–∏ –∑ –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤.")

                    # 3. –ó–∞–≥–∞–ª—å–Ω–∏–π –ø—ñ–¥—Å—É–º–æ–∫ –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤
                    st.markdown("##### üìù –ó–∞–≥–∞–ª—å–Ω–∏–π –ø—ñ–¥—Å—É–º–æ–∫ –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤ (–∑–∞ –≤–µ—Ä—Å—ñ—î—é GPT):")
                    with st.spinner("GPT –≥–µ–Ω–µ—Ä—É—î –ø—ñ–¥—Å—É–º–æ–∫ –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤..."):
                        summary_from_gpt = gpt_comment_summary(comment_texts_for_gpt)
                    st.markdown(summary_from_gpt)

                    # 4. –¢–æ–ø-10 –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤ –∑ –∞–Ω–∞–ª—ñ–∑–æ–º –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—ñ
                    st.markdown("##### üî• –¢–æ–ø-10 –Ω–∞–π–ø–æ–ø—É–ª—è—Ä–Ω—ñ—à–∏—Ö –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤ (–∑–∞ –ª–∞–π–∫–∞–º–∏) —Ç–∞ –∞–Ω–∞–ª—ñ–∑ —ó—Ö –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—ñ:")
                    comments_with_likes = [c for c in fetched_comments_data if isinstance(c.get('likes'), int)]
                    if not comments_with_likes:
                        st.info("–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤ –∑ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—î—é –ø—Ä–æ –ª–∞–π–∫–∏ –¥–ª—è –≤—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è —Ç–æ–ø—É.")
                    else:
                        top_10_comments = sorted(comments_with_likes, key=lambda x: x['likes'], reverse=True)[:10]
                        for i, comment_detail in enumerate(top_10_comments):
                            st.markdown(f"---\n**{i + 1}.** üëç {comment_detail['likes']:,} –ª–∞–π–∫—ñ–≤")
                            st.markdown(f"> {comment_detail['text']}")
                            
                            # –ê–Ω–∞–ª—ñ–∑ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—ñ –æ–∫—Ä–µ–º–æ–≥–æ –∫–æ–º–µ–Ω—Ç–∞—Ä—è
                            replies_texts = [r.get("snippet", {}).get("textDisplay", "") for r in comment_detail.get("replies", []) if r.get("snippet", {}).get("textDisplay")]
                            with st.spinner(f"GPT –∞–Ω–∞–ª—ñ–∑—É—î –ø–æ–ø—É–ª—è—Ä–Ω—ñ—Å—Ç—å –∫–æ–º–µ–Ω—Ç–∞—Ä—è #{i+1}..."):
                                popularity_analysis = gpt_analyze_comment_popularity(comment_detail['text'], replies_texts)
                            st.markdown(f"<small style='color:grey;'><i><b>–ê–Ω–∞–ª—ñ–∑ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—ñ –≤—ñ–¥ GPT:</b> {popularity_analysis}</i></small>", unsafe_allow_html=True)

                            # –í—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π (—è–∫—â–æ —î)
                            replies_list = comment_detail.get("replies", [])
                            if replies_list:
                                valid_replies = [
                                    r for r in replies_list if isinstance(r, dict) and 
                                    isinstance(r.get("snippet", {}).get("likeCount"), int) and
                                    r.get("snippet", {}).get("textDisplay")]
                                sorted_replies = sorted(valid_replies, key=lambda r_item: r_item["snippet"]["likeCount"], reverse=True)[:3]
                                if sorted_replies:
                                    with st.expander(f"üí¨ –ü–æ–∫–∞–∑–∞—Ç–∏ –¥–æ {len(sorted_replies)} –Ω–∞–π–ø–æ–ø—É–ª—è—Ä–Ω—ñ—à–∏—Ö –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π"):
                                        for reply_item in sorted_replies:
                                            reply_snippet = reply_item.get("snippet", {})
                                            st.markdown(f"&nbsp;&nbsp;‚Ü≥ {reply_snippet.get('textDisplay', '')} _(üëç {reply_snippet.get('likeCount', 0):,})_")
